{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install and Import packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e83f06c1de74b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (0.16.5)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (5.9.0)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (1.44.0)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: PyYAML in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: setproctitle in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (68.2.2)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (1.4.4)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from wandb) (4.25.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/fengli/anaconda3/envs/from_scratch/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.478178Z",
     "start_time": "2024-04-11T19:51:54.615661Z"
    }
   },
   "id": "be348e0bfb157c",
   "execution_count": 455
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "from datetime import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from transformers import AutoTokenizer\n",
    "import collections\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from typing import List, Optional, Tuple, Union"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.483521Z",
     "start_time": "2024-04-11T19:51:56.479630Z"
    }
   },
   "id": "afc815861d75e9ce",
   "execution_count": 456
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4c44fb48d262e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# check if the system is running on colab or macbook m1 pro\n",
    "import platform\n",
    "import os\n",
    "op_system = platform.system()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.486916Z",
     "start_time": "2024-04-11T19:51:56.484416Z"
    }
   },
   "id": "71684418f401d651",
   "execution_count": 457
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b368a97601cce09"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if op_system == 'Darwin':\n",
    "    # Macbook\n",
    "    data_path = '../../data/translation/wmt14-en-de/'\n",
    "elif op_system == 'Linux':\n",
    "    # Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    data_path = '/content/drive/MyDrive/Projects/nlp_emotion/data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.490896Z",
     "start_time": "2024-04-11T19:51:56.488283Z"
    }
   },
   "id": "abee6ce01d812c0a",
   "execution_count": 458
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4963a316123c0d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# REPORT_WANDB = True\n",
    "REPORT_WANDB = False\n",
    "run_name = \"self_implemented_transformer_not_converging\"\n",
    "\n",
    "check_point_folder_path = data_path + \"/check_point\"\n",
    "\n",
    "device_type = \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    device_type = \"mps\"\n",
    "elif op_system == 'Darwin':\n",
    "    device_type = \"cuda\"\n",
    "device = torch.device(device_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32 if device_type == \"mps\" else 12\n",
    "SEQ_LEN = 64 if device_type == \"mps\" else 512\n",
    "ENCODER_LAYER_NUM = 6\n",
    "DECODER_LAYER_NUM = 6\n",
    "D_MODEL = 256 if device_type == \"mps\" else 512\n",
    "HIDDEN_DIM = 512 if device_type == \"mps\" else 2048\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",pad_token=\"<pad>\",bos_token=\"<sos>\",eos_token=\"<eos>\",\n",
    "                                                       add_bos_token=True, add_eos_token=True,max_length=SEQ_LEN, padding=\"max_length\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EPOCHS = 50 if device_type == \"mps\" else 3\n",
    "STEPS = 1000000\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.98\n",
    "EPSILON = 1e-9\n",
    "LEARNING_RATE = 0.00001\n",
    "WARMUP_STEPS = 4000\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.671372Z",
     "start_time": "2024-04-11T19:51:56.491472Z"
    }
   },
   "id": "10292570894400b0",
   "execution_count": 459
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.674023Z",
     "start_time": "2024-04-11T19:51:56.672029Z"
    }
   },
   "id": "6c7f94a7255ff990",
   "execution_count": 460
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7df1049127ff6ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd5e5347d48da09"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WMT14ENDEDatasetHuggingFace(data.Dataset):\n",
    "\n",
    "    def __init__(self, en_raw_file_path= \"\",de_raw_file_path=\"\",\n",
    "               max_len=512, device=\"cuda\",data_size:str=\"all\"):\n",
    "        self.device = device\n",
    "        if data_size == \"all\":\n",
    "            data_size = \"all\"\n",
    "        else:\n",
    "            data_size = int(data_size)\n",
    "        with open(en_raw_file_path, \"r\") as f:\n",
    "            if data_size != \"all\":\n",
    "                en_sentence = f.readlines()[:data_size]\n",
    "            else:\n",
    "                en_sentence = f.readlines()\n",
    "        with open(de_raw_file_path, \"r\") as f:\n",
    "            if data_size !=\"all\":\n",
    "                de_sentence = f.readlines()[:data_size]\n",
    "            else:\n",
    "                de_sentence = f.readlines()\n",
    "        assert len(en_sentence) == len(de_sentence), \"The number of english and german sentences should be the same\"\n",
    "        self.data = list(zip(en_sentence, de_sentence))\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",pad_token=\"<pad>\",bos_token=\"<sos>\",eos_token=\"<eos>\",\n",
    "                                                       add_bos_token=True, add_eos_token=True,max_length=max_len)\n",
    "        self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\": \"<sos>\", \"eos_token\": \"<eos>\"})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_sentence_str,de_sentence_str = self.data[idx]\n",
    "        en_sentence_str = \"<sos> \" + en_sentence_str.strip() + \" <eos>\"\n",
    "        de_sentence_str = \"<sos> \" + de_sentence_str.strip() + \" <eos>\"\n",
    "        \n",
    "        # run huggingface tokenizer\n",
    "        en_sentence = self.tokenizer(en_sentence_str, padding=\"max_length\", truncation=True,\n",
    "                                     max_length=self.max_len, return_tensors=\"pt\",add_special_tokens=True)\n",
    "        de_sentence = self.tokenizer(de_sentence_str, padding=\"max_length\", truncation=True,\n",
    "                                     max_length=self.max_len, return_tensors=\"pt\",add_special_tokens=True\n",
    "                                     )\n",
    "\n",
    "\n",
    "        en_sentence_id = en_sentence[\"input_ids\"].squeeze().to(self.device)\n",
    "        de_sentence_id = de_sentence[\"input_ids\"].squeeze().to(self.device)\n",
    "        en_padding_mask = en_sentence[\"attention_mask\"].squeeze().to(self.device)\n",
    "        de_padding_mask = de_sentence[\"attention_mask\"].squeeze().to(self.device)\n",
    "        return {\n",
    "            \"en_input_ids\": en_sentence_id,\n",
    "            \"de_input_ids\": de_sentence_id,\n",
    "            \"en_padding_mask\": en_padding_mask,\n",
    "            \"de_padding_mask\": de_padding_mask,\n",
    "            \"en_sentence_str\": en_sentence_str,\n",
    "            \"de_sentence_str\": de_sentence_str\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.680731Z",
     "start_time": "2024-04-11T19:51:56.674727Z"
    }
   },
   "id": "61a7bea559c8ef2e",
   "execution_count": 461
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation BLEU Score\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cecf33a22ba5f1dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "    \"\"\"\n",
    "    Extracts all n-grams up to a given maximum order from an input segment.\n",
    "\n",
    "    :param segment:\n",
    "        text segment from which n-grams will be extracted\n",
    "        list of tokens\n",
    "        [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "    :param max_order:\n",
    "        maximum length of n-grams\n",
    "    :return:\n",
    "        a Counter with n-gram counts\n",
    "    \"\"\"\n",
    "    # create a counter to store the n-gram counts\n",
    "    ngram_counts = collections.Counter()\n",
    "    # run through all the n-gram from 1 to max_order\n",
    "    for order in range(1, max_order + 1):\n",
    "        # run through all the n-gram in the segment\n",
    "        for i in range(len(segment) - order + 1):\n",
    "            # get the n-gram, need to convert the n-gram to tuple since list is not hashable\n",
    "            ngram = tuple(segment[i:i + order])\n",
    "            # increment the n-gram count\n",
    "            ngram_counts[ngram] += 1\n",
    "    # return the n-gram counts, in which the key is the form 1 to max_order n-gram, the value is the frequency of the\n",
    "    # n-gram\n",
    "    return ngram_counts\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False, smooth_value=0.0):\n",
    "    \"\"\"\n",
    "     Implementation of BLEU score.\n",
    "    :param reference_corpus:\n",
    "        list of list of reference sentences, each sentence is a list of tokens\n",
    "        1 st level: number of \"list of reference sentences\", len is the number of translations\n",
    "        [\n",
    "            [\n",
    "                [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"],\n",
    "                [\"token1\", \"token2\", \"token3\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"token1\", \"token2 \", \"token3\", \"token4\", \"token5\"],\n",
    "                [\"token1\", \"token2\", \"token3\"]\n",
    "            ]\n",
    "        ]\n",
    "        2 nd level: number of \"reference sentences\" for a single translation, len is the number of references\n",
    "        [\n",
    "            [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"],\n",
    "            [\"token1\", \"token2\", \"token3\"]\n",
    "        ]\n",
    "        3 rd level: number of tokens in a single reference sentence, len is the number of tokens in a single sentence\n",
    "        [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "    :param translation_corpus:\n",
    "        list of translated sentences, each sentence is a list of tokens, those sentences are the predicted sentences\n",
    "        that we want to evaluate\n",
    "        1 st level: number of \"list of translated sentences\", len is the number of translations\n",
    "        [\n",
    "            [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"],\n",
    "            [\"token1\", \"token2\", \"token3\"]\n",
    "        ]\n",
    "        2 nd level: number of tokens in a single translated sentence, len is the number of tokens in a single sentence\n",
    "        [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "    :param max_order:\n",
    "        the maximum n-gram order to use when computing BLEU score, usually 4\n",
    "    :param smooth:\n",
    "        whether to apply smoothing, default is False, if do not apply smoothing, then the n-gram modified\n",
    "        precision will be 0 if there is no n-gram overlap, that will make the log of 0, which is undefineda\n",
    "    :param smooth_value:\n",
    "        the value to use when applying smoothing, default is 0.0\n",
    "    :return:\n",
    "        the BLEU score, the value is between 0 and 1, the higher, the better\n",
    "    \"\"\"\n",
    "\n",
    "    matches_by_order = [0] * max_order\n",
    "    possible_matches_by_order = [0] * max_order\n",
    "    reference_length = 0\n",
    "    translation_length = 0\n",
    "\n",
    "    for (references, translation) in zip(reference_corpus, translation_corpus):\n",
    "        # when compute the brevity penalty, we have to consider the shortest reference sentence\n",
    "        # for the translation sentences, we need to add them up\n",
    "        reference_length += min(len(r) for r in references)\n",
    "        translation_length += len(translation)\n",
    "\n",
    "        # create a counter to store the n-gram counts of the merged reference sentences\n",
    "        merged_ref_ngram_counts = collections.Counter()\n",
    "        for reference in references:\n",
    "            # compute the n-gram counts of every reference sentence, and merge them\n",
    "            # the merge is not accumulative, it is the keep the maximum count of the n-gram\n",
    "            # for counter, the + operator will sum the count of the same key, where the | operator will keep the maximum\n",
    "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "        # get the n-gram counts of the translation sentence\n",
    "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "        # get the n-gram overlap of the translation sentence and the merged reference sentences\n",
    "        # the & operator will return the minimum count of the n-gram, if the n-gram is not in the merged reference\n",
    "        # sentences, then the count will be 0\n",
    "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "\n",
    "        for ngram in overlap:\n",
    "            # increment the n-gram overlap count\n",
    "            # len(ngram) is to calculate the order of the n-gram, since the n-gram is a tuple, the length of the tuple\n",
    "            # minus 1 will be the order of the n-gram\n",
    "            matches_by_order[len(ngram) - 1] += overlap[ngram]\n",
    "        for order in range(1, max_order + 1):\n",
    "            # when calculate precision of the n-gram, we have to consider the possible matches as the denominator\n",
    "            # if the sentence len is x, then the possible n-gram is x - order + 1\n",
    "            possible_matches = len(translation) - order + 1\n",
    "            # to avoid the division by 0, we have to check if the possible matches is greater than 0,\n",
    "            # that only happens when the order is greater than the length of the sentence\n",
    "            # for example, if the sentence is [\"the\", \"cat\"], the possible bigram is 1, the possible trigram is 0\n",
    "            if possible_matches > 0:\n",
    "                # increment the possible n-gram matches count\n",
    "                possible_matches_by_order[order - 1] += possible_matches\n",
    "        precision = [0] * max_order\n",
    "        for i in range(0,max_order):\n",
    "            if smooth:\n",
    "                # if one of the n-gram order has no possible matches, then the precision will be 0\n",
    "                # but we have to avoid the division by 0, so we have to add a smooth value\n",
    "                precision[i] = (matches_by_order[i] + smooth_value) / (possible_matches_by_order[i] + smooth_value)\n",
    "            else:\n",
    "                if possible_matches_by_order[i] > 0:\n",
    "                    precision[i] = matches_by_order[i] / possible_matches_by_order[i]\n",
    "                else:\n",
    "                    precision[i] = 0\n",
    "\n",
    "        if min(precision) > 0:\n",
    "            # the reason using geometric mean is that the n-gram precision is highly correlated\n",
    "            # if they are independent, then we could use the arithmetic mean, but if the triple-gram precision is high,\n",
    "            # then the bigram precision will be high, so we have to use the geometric mean\n",
    "\n",
    "            # but using geometric mean will make the result underflow, since the precision is between 0 and 1\n",
    "            # we will multiply a number between 0 and 1 multiple times, the result will be really small\n",
    "            # so we have to use the log to avoid the underflow\n",
    "            # and at the end, we have to use exp to get the final result back, since log then exp will cancel each other\n",
    "            p_log_sum = sum((1 / max_order) * math.log(p) for p in precision)\n",
    "            geo_mean = math.exp(p_log_sum)\n",
    "        else:\n",
    "            geo_mean = 0\n",
    "\n",
    "        # compute the brevity penalty\n",
    "        ratio =  float(translation_length) / reference_length\n",
    "        if ratio > 1.0:\n",
    "            bp = 1\n",
    "        else:\n",
    "            bp = math.exp(1 - 1.0 / ratio)\n",
    "        bleu = geo_mean * bp\n",
    "    return {\"bleu\": bleu, \"geo_mean\": geo_mean, \"bp\": bp,\"unigram\": precision[0], \"bigram\": precision[1],\n",
    "            \"trigram\": precision[2], \"fourgram\": precision[3]}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.690253Z",
     "start_time": "2024-04-11T19:51:56.681609Z"
    }
   },
   "id": "7760449b72ef49c1",
   "execution_count": 462
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53f9770c1d8e0971"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def clone(component: nn.Module, num_of_copy: int) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    In the transformer structure, there will a lot of repeat component, for example, the identical layer of encoders and\n",
    "    decoders. In order to create those identical components, we will need this clone function to create a list ModuleList\n",
    "    :param component: the component will be copied\n",
    "    :param num_of_copy: the number of copies will be in the final module list\n",
    "    :return: a module list contain num_of_copy component\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(component) for _ in range(num_of_copy)])\n",
    "\n",
    "\n",
    "def _get_padding_mask(attention_mask: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The padding mask is used to prevent the model to look at the padding token\n",
    "    :param attention_mask:\n",
    "        the mask is generated by tokenizer, usually the dim is [batch_size * seq_len] contains of 1 and 0, where 1\n",
    "        represent the position of the corresponding sentence is a meaningful token, otherwise it is a padding.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create padding mask\n",
    "    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "\n",
    "    attention_mask = attention_mask[:, None, None, :]\n",
    "    attention_mask = 1.0 - attention_mask\n",
    "    attention_mask = attention_mask.masked_fill(attention_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "    return attention_mask\n",
    "\n",
    "\n",
    "def _get_causal_mask(attention_mask: torch.Tensor, input_shape: Tuple[int, int], dtype: torch.dtype,\n",
    "                     ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The causal mask is used in decoder, the mask is used to prevent the model to look ahead the future token\n",
    "    :param attention_mask:\n",
    "        the mask is generated by tokenizer, usually the dim is [batch_size * seq_len] contains of 1 and 0, where 1\n",
    "        represent the position of the corresponding sentence is a meaningful token, otherwise it is a padding.\n",
    "    :param input_shape:\n",
    "        the shape of the input tensor, tuple of batch_size and seq_len of decoder input\n",
    "    :param inputs_embeds:\n",
    "        the embedding of the input tensor, the dim of the input tensor is [batch_size * seq_len * d_model]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # add the past_key_values_length to the seq_len of the input tensor\n",
    "    key_value_length = input_shape[-1]\n",
    "\n",
    "    # 4d mask is passed through the layers\n",
    "    # if the attention_mask is 2D,\n",
    "\n",
    "    input_shape = (attention_mask.shape[0], input_shape[-1])\n",
    "\n",
    "    # create causal mask\n",
    "    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "    causal_4d_mask = None\n",
    "    if input_shape[-1] > 1:\n",
    "        if key_value_length is None:\n",
    "            raise ValueError(\n",
    "                \"This attention mask converter is causal. Make sure to pass `key_value_length` to correctly create a causal mask.\"\n",
    "            )\n",
    "\n",
    "        bsz, tgt_len = input_shape\n",
    "        # create a mat that have the same size as attention weight\n",
    "        mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=attention_mask.device)\n",
    "        # rang a one dim mat only on conditional\n",
    "        mask_cond = torch.arange(mask.size(-1), device=attention_mask.device)\n",
    "        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "\n",
    "        mask = mask.to(dtype)\n",
    "\n",
    "        causal_4d_mask = mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len)\n",
    "\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "\n",
    "        bsz, src_len = attention_mask.size()\n",
    "        tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "        expanded_mask = attention_mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "        inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "        expanded_attn_mask = inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "        if causal_4d_mask is not None:\n",
    "            expanded_attn_mask = causal_4d_mask.masked_fill(expanded_attn_mask.bool(), torch.finfo(dtype).min)\n",
    "\n",
    "        # expanded_attn_mask + causal_4d_mask can cause some overflow\n",
    "        expanded_4d_mask = expanded_attn_mask\n",
    "\n",
    "    return expanded_4d_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.696593Z",
     "start_time": "2024-04-11T19:51:56.691123Z"
    }
   },
   "id": "3c16910aff5971fe",
   "execution_count": 463
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Config Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75764cb9bd3486ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    \"\"\"\n",
    "    The config class is an easy way to parse those hyper params into model\n",
    "    Since nowadays model architecture could be really deep, packing all the hypers into one config obj and pass this obj\n",
    "    from one component to deeper component is more neat than every __init__ func have a bunch of param\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int = 512,\n",
    "                 num_heads: int = 8,\n",
    "                 dropout: float = 0.1,\n",
    "                 batch_size: int = 16,\n",
    "                 seq_len: int = 256,\n",
    "                 d_ff: int = 2048,\n",
    "                 vocab_size: int = 37000,\n",
    "                 device: str = \"cuda\",\n",
    "                 encoder_layer_num: int = 6,\n",
    "                 decoder_layer_num: int = 6,\n",
    "                 eps: float = 1e-6\n",
    "                 ):\n",
    "        # the main model size of the transformer model, in the whole model,  we will use d_model number vector to\n",
    "        # represent meaning of the word (the location of this word in the word embedding space)\n",
    "        self.d_model = d_model\n",
    "        # number of the heads define when we do the attention operation, parallel, there will be [num_heads] heads using\n",
    "        # the same inputs but different learnable params to the same operation, the concat res will be the final res of\n",
    "        # attention operation\n",
    "        self.num_heads = num_heads\n",
    "        # the dropout layer is critical in deep learning model, dropout is fantastic technical that can proven the\n",
    "        # model overfit. what the dropout layer doing is it \"cover/cut\" random a percentage of input when it is\n",
    "        # running, so the model won't over-relay on a certain feature/path of the model. it will increase the\n",
    "        # robustness of the model\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        # seq_len is the max number of token the model could process in one operation, not like RNN the model process\n",
    "        # the input token by token, all the attention computation in transformer could be done at teh same time, we have\n",
    "        # to define the max number of token, so the model can create weight mat accordingly\n",
    "        self.seq_len = seq_len\n",
    "        # the inner layer dim of fully-connected feed-forward component\n",
    "        self.d_ff = d_ff\n",
    "        # the vocab size of the tokenized, will be used to generate embedding layer and final fully connected layer\n",
    "        self.vocab_size = vocab_size\n",
    "        # indicate where the whole model will be running, all the tensor involved in the computation need to be moved\n",
    "        # on the same device\n",
    "        self.device = device\n",
    "        self.encoder_layer_num = encoder_layer_num\n",
    "        self.decoder_layer_num = decoder_layer_num\n",
    "        self.eps = eps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.702627Z",
     "start_time": "2024-04-11T19:51:56.698978Z"
    }
   },
   "id": "b6ae951e8256481c",
   "execution_count": 464
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiHeadAttention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eceba978435e2d8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi Head attention is a foundation component of transformer model,\n",
    "    What is does just repeat the scaled dot product attention operation several times parallel, each time we call it a\n",
    "    Head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig, is_cross: bool = False):\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # since those head are doing the attention operation at the same time, we better put them in a same matrix\n",
    "        # to make it efficient. In that case, if we define single head dim as d_single_head\n",
    "        # d_model = num_heads * d_single_head. before we do the scaled dot-product we have to assert, otherwise we can't\n",
    "        # split the d_model evenly into heads\n",
    "        self.d_model = config.d_model\n",
    "        self.num_heads = config.num_heads\n",
    "        assert self.d_model % self.num_heads == 0, \"the number of head need to be divided by d_model\"\n",
    "        # this linear nn.ModuleList contains the W_q,W_k,W_v,W_o. All of them have the same size the purpose the W_q,\n",
    "        # W_k,W_v is for projection. to do the scale dot-production attention, we have to use query(q) * key(k) to\n",
    "        # get score between q and k then use the score as weight to retrieve info from the v, but there is an issue,\n",
    "        # the original input the attention is general. For example in self attention, the original input of attention\n",
    "        # is the same, 3 identical matrix represent a general meaning of the sentence. to get a better result. We\n",
    "        # want project the general meaning into a specific space (query space, key space and value space) and use\n",
    "        # those projected(professional) value to do the scale dot-product this W_o is used when we concat and\n",
    "        # aggregate each head's value into final attention res since those head might have the same result,\n",
    "        # some may focus on less important relation between q and k, we need a learnable params to assign weight to\n",
    "        # each head and their dim\n",
    "        self.linears = clone(nn.Linear(self.d_model, self.d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.seq_len = config.seq_len\n",
    "        self.d_k = self.d_model // self.num_heads\n",
    "        self.is_cross = is_cross\n",
    "\n",
    "    def _scaled_dot_product(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n",
    "                            mask: torch.Tensor, dropout: nn.Dropout) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this function will actually do the scaled dot product mentioned in equation (1)\n",
    "        for this function, all q k v need to be prepared, which it has already been split into head dim\n",
    "        for the mask, it also should adjust to proper dim for broadcasting operation\n",
    "        :param q: [batch_size * num_heads * seq_len * d_k]\n",
    "        :param k: [batch_size * num_heads * seq_len * d_k]\n",
    "        :param v: [batch_size * num_heads * seq_len * d_k]\n",
    "        :param mask: [ batch_size * 1 * seq_len * 1]\n",
    "        :param dropout: the dropout defined in the outer layer\n",
    "        :return: the scaled dot-product result [batch_size * num_heads * seq_len * d_k]\n",
    "        \"\"\"\n",
    "        d_k = q.size(-1)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # TODO explain the mask fill and why there is a small value\n",
    "            scores = scores + mask\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        # TODO explain why dropout before\n",
    "        return torch.matmul(scores, v)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this function implement the function in section 3.2.2\n",
    "        :param q:[batch_size * seq_len * d_model]\n",
    "        :param k:[batch_size * seq_len * d_model]\n",
    "        :param v:[batch_size * seq_len * d_model]\n",
    "        :param mask:[batch_size * seq_len]\n",
    "        :param is_decoder: if the component is decoder, the mask should be different\n",
    "        :return: result of multi head attention [batch_size * seq_len * d_model]\n",
    "        \"\"\"\n",
    "        # get the batch since the q k v need to be reshaped latter. the number of sentence in the batch won't be all the\n",
    "        # time the same, for example, the last batch may not be full\n",
    "        batch_size = q.size(0)\n",
    "        # TODO explain why have mask here\n",
    "\n",
    "        # the mask is generated by tokenizer, usually the dim is [batch_size * seq_len] contains of 1 and 0\n",
    "        # where 1 represent the position of the corresponding sentence is a meaningful token, otherwise it is a\n",
    "        # padding. in order to use it, mask_fill the score, it has to meet the requirement of broadcasting with score\n",
    "        # since the dim of score is [batch_size * num_heads * seq_len * d_k], the mask has to un-squeeze at dim 1 and\n",
    "        # dim -1\n",
    "        if mask is not None:\n",
    "            if self.is_cross and self.training:\n",
    "                # get the causal mask, the mask is used to prevent the model to look ahead the future token\n",
    "                # the dim of the mask is already [ 1 * 1 * seq_len * seq_len]\n",
    "                # the shape of the mask is [batch_size * 1 * key_len * memory_len]\n",
    "\n",
    "                mask = _get_causal_mask(attention_mask=mask,\n",
    "                                        input_shape = mask.size(),\n",
    "                                        dtype=q.dtype)\n",
    "\n",
    "            else:\n",
    "                # the mask is used to prevent the model to look at the padding token\n",
    "                # the dim of the mask is already [batch_size * 1 * seq_len * 1]\n",
    "                if not self.training:\n",
    "                    mask = _get_padding_mask(attention_mask=mask, dtype=q.dtype)\n",
    "                else:\n",
    "                    mask = _get_padding_mask(attention_mask=mask, dtype=q.dtype)\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for l, x in\n",
    "                             zip(self.linears, (q, k, v))]\n",
    "        x = self._scaled_dot_product(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # project the input q,k,v into according space to get actual query, key and value\n",
    "        # q, k, v = [w(mat) for mat, w in zip([q, k, v], self.linears)]\n",
    "        # reshape the q , k  and v to into heads, constitute the multi head\n",
    "        # q, k, v = [mat.view(batch_size, self.seq_len, self.num_heads, -1) for mat in [q, k, v]]\n",
    "        # transpose the number since the matmul only work on last two dim, to calculate the attention, we want to\n",
    "        # compute q [...... seq_len * d_q] * k [..... d_k, seq_len]\n",
    "        # after reshaping, the dim is [batch_size * seq_len, num_heads, d_k]\n",
    "        # so dim 1 and dim 2 need to transpose\n",
    "        # q, k, v = [torch.transpose(mat, 1, 2) for mat in [q, k, v]]\n",
    "        # after everything be prepared, the scaled dot-product will be conducted.\n",
    "        # the output of that func is split into head, we need transpose the dim back and reshape the same dim\n",
    "        # as the input, so in the transformer the following identical layer could keeping do the same attention\n",
    "        # operation again and again\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        # TODO why need contiguous()\n",
    "        # x = (self._scaled_dot_product(q=q, k=k, v=v, mask=mask, dropout=self.dropout)\n",
    "        #      .transpose(1, 2).contiguous().view(batch_size, -1, self.d_model))\n",
    "        return self.linears[-1](x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.711628Z",
     "start_time": "2024-04-11T19:51:56.703689Z"
    }
   },
   "id": "7c7bff711b75f437",
   "execution_count": 465
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FeedForward Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a55f1931832f4e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    # todo what is purpose this feed forward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.w_2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this function implement the equation (2)\n",
    "        :param x: [batch_size * seq_len * d_model]\n",
    "        :return: [batch_size * seq_len * d_model]\n",
    "        \"\"\"\n",
    "        # TODO explain the position of the dropout\n",
    "        # according to the equation, this fully connected feed forward layer, this consists of two linear\n",
    "        # transformations with a ReLU activation in between.\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.715047Z",
     "start_time": "2024-04-11T19:51:56.712382Z"
    }
   },
   "id": "30ebedb53e209bf9",
   "execution_count": 466
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b379fe4d2a5c651a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.d_model = torch.tensor(config.d_model).to(config.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(x) * torch.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Todo\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # todo\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        pe = torch.zeros(config.seq_len, config.d_model)\n",
    "        position = torch.arange(0, config.seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, config.d_model, 2) * -(math.log(10000.0) / config.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = embedding + Variable(self.pe[:, :embedding.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.719741Z",
     "start_time": "2024-04-11T19:51:56.715792Z"
    }
   },
   "id": "ac5d3b38fc40ef03",
   "execution_count": 467
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LayerNorm Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84da03ea9015000"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.one_mat = nn.Parameter(torch.ones(config.d_model))\n",
    "        self.zero_mat = nn.Parameter(torch.zeros(config.d_model))\n",
    "        self.eps = config.eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.one_mat * (x - mean) / (std + self.eps) + self.zero_mat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.723437Z",
     "start_time": "2024-04-11T19:51:56.720465Z"
    }
   },
   "id": "73d23af9074c6e21",
   "execution_count": 468
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sublayer Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79d5c7f6920785f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class Sublayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(Sublayer, self).__init__()\n",
    "        self.norm = LayerNorm(config)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, module: nn.Module) -> torch.Tensor:\n",
    "\n",
    "        return self.dropout(module(self.norm(x))) + x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.726794Z",
     "start_time": "2024-04-11T19:51:56.724392Z"
    }
   },
   "id": "1752a1b77c970e0",
   "execution_count": 469
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EncoderLayer Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d89d82402be641e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.sublayers = clone(Sublayer(config), 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_masking: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attention(x, x, x, src_masking))\n",
    "        x = self.sublayers[1](x, self.ffn)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.730016Z",
     "start_time": "2024-04-11T19:51:56.727452Z"
    }
   },
   "id": "8a55c7954e0b6ee0",
   "execution_count": 470
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "227ad46f3b175254"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layer_list = clone(EncoderLayer(config), config.encoder_layer_num)\n",
    "        self.norm = LayerNorm(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_masking: torch.Tensor) -> torch.Tensor:\n",
    "        for encoder_layer in self.encoder_layer_list:\n",
    "            x = encoder_layer(x, src_masking)\n",
    "        return self.norm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.733083Z",
     "start_time": "2024-04-11T19:51:56.730754Z"
    }
   },
   "id": "d777c91e1ecb2f65",
   "execution_count": 471
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DecoderLayer Class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b680dfdf1e4666f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.cross_attention = MultiHeadAttention(config, is_cross=True)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.sublayers = clone(Sublayer(config), 3)\n",
    "\n",
    "    def forward(self, memory: torch.Tensor, x: torch.Tensor, src_masking: torch.Tensor,\n",
    "                tgt_masking: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attention(x, x, x, tgt_masking))\n",
    "        if self.training:\n",
    "            x = self.sublayers[1](x, lambda x: self.cross_attention(x, memory, memory, src_masking))\n",
    "        else:\n",
    "            x = self.sublayers[1](x, lambda x: self.cross_attention(x, memory, memory, src_masking))\n",
    "        x = self.sublayers[2](x, self.ffn)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.737040Z",
     "start_time": "2024-04-11T19:51:56.733993Z"
    }
   },
   "id": "c99b00af3cb57c96",
   "execution_count": 472
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder Class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc87e56c157b657"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layer_list = clone(DecoderLayer(config), config.decoder_layer_num)\n",
    "        self.norm = LayerNorm(config)\n",
    "\n",
    "    def forward(self, memory: torch.Tensor, x: torch.Tensor,\n",
    "                src_masking: torch.Tensor, tgt_masking: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        for decoder_layer in self.decoder_layer_list:\n",
    "            x = decoder_layer(memory, x, src_masking, tgt_masking)\n",
    "        return self.norm(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.740269Z",
     "start_time": "2024-04-11T19:51:56.737799Z"
    }
   },
   "id": "932429a5ed0aad5f",
   "execution_count": 473
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1efb034794efc1df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = Embedding(config)\n",
    "        self.pe = PositionalEmbedding(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.linear = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "    def forward(self, src_x: torch.Tensor, tgt_x: torch.Tensor,\n",
    "                src_masking: torch.Tensor, tgt_masking: torch.Tensor) -> torch.Tensor:\n",
    "        src_embedding = self.embedding(src_x)\n",
    "        tgt_embedding = self.embedding(tgt_x)\n",
    "        src_pe = self.pe(src_embedding)\n",
    "        tgt_pe = self.pe(tgt_embedding)\n",
    "\n",
    "        memory = self.encoder(src_pe, src_masking)\n",
    "\n",
    "        output = self.decoder(memory, tgt_pe, src_masking, tgt_masking)\n",
    "\n",
    "        logits = F.log_softmax(self.linear(output), dim=-1)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.744388Z",
     "start_time": "2024-04-11T19:51:56.740873Z"
    }
   },
   "id": "1da0b9eaa0af98f",
   "execution_count": 474
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUN "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f98bbbd519a64f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7da3f73a4ad84d59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(check_point_folder_path):\n",
    "    os.makedirs(check_point_folder_path)\n",
    "\n",
    "if op_system == \"Darwin\":\n",
    "   train_data_size = \"1000\"\n",
    "else:\n",
    "    train_data_size = \"all\"\n",
    "\n",
    "checkpoint_files = os.listdir(check_point_folder_path)\n",
    "checkpoint_file_name = f\"checkpoint_{train_data_size}_batch_size-{BATCH_SIZE}_seq_len-{SEQ_LEN}_encoder_layer_num-{ENCODER_LAYER_NUM}_decoder_layer_num-{DECODER_LAYER_NUM}_d_model-{D_MODEL}_hidden_dim-{HIDDEN_DIM}_num_heads-{NUM_HEADS}_dropout-{DROPOUT}_vocab_size-{VOCAB_SIZE}_epochs-{EPOCHS}_steps-{STEPS}_beta1-{BETA1}_beta2-{BETA2}_epsilon-{EPSILON}_learning_rate-{LEARNING_RATE}_warmup_steps-{WARMUP_STEPS}\"\n",
    "if checkpoint_file_name in checkpoint_files:\n",
    "    # load the model from the checkpoint\n",
    "    transformer = torch.load(check_point_folder_path + \"/\" + checkpoint_file_name)\n",
    "else:\n",
    "\n",
    "\n",
    "    if REPORT_WANDB:\n",
    "        wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"from_scratch_vanilla_transformer_debug\",\n",
    "        name=run_name,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"seq_len\": SEQ_LEN,\n",
    "            \"encoder_layer_num\": ENCODER_LAYER_NUM,\n",
    "            \"decoder_layer_num\": DECODER_LAYER_NUM,\n",
    "            \"d_model\": D_MODEL,\n",
    "            \"hidden_dim\": HIDDEN_DIM,\n",
    "            \"num_heads\": NUM_HEADS,\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"vocab_size\": VOCAB_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"steps\": STEPS,\n",
    "            \"beta1\": BETA1,\n",
    "            \"beta2\": BETA2,\n",
    "            \"epsilon\": EPSILON,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"warmup_steps\": WARMUP_STEPS,\n",
    "            \"device\": device.type,\n",
    "            \"timestamp\": time()\n",
    "        }\n",
    "        )\n",
    "\n",
    "    transformer_config = TransformerConfig(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seq_len=SEQ_LEN,\n",
    "        encoder_layer_num=ENCODER_LAYER_NUM,\n",
    "        decoder_layer_num=DECODER_LAYER_NUM,\n",
    "        d_model=D_MODEL,\n",
    "        d_ff=HIDDEN_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        device=device,\n",
    "        eps = 1e-6,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:51:56.749392Z",
     "start_time": "2024-04-11T19:51:56.745010Z"
    }
   },
   "id": "dd502d344cb1a83b",
   "execution_count": 475
  },
  {
   "cell_type": "markdown",
   "source": [
    "## initialize the model and dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d48e81cd09f37cdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "transformer = Transformer(transformer_config)\n",
    "transformer.to(device)\n",
    "\n",
    "# adam with beta1 = 0.9, beta2 = 0.98, epsilon = 1e-9\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2), eps=EPSILON)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100000, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "wmt14_en_de_tokenizer_dataset = WMT14ENDEDatasetHuggingFace(\n",
    "    en_raw_file_path= data_path + \"raw/train/train.en\",\n",
    "    de_raw_file_path= data_path + \"raw/train/train.de\",\n",
    "    device=device, max_len=SEQ_LEN, data_size=train_data_size)\n",
    "\n",
    "dataloader = DataLoader(wmt14_en_de_tokenizer_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "epoch_loss_list = []\n",
    "step_loss_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:52:00.564739Z",
     "start_time": "2024-04-11T19:51:56.749999Z"
    }
   },
   "id": "4b4503b63a81adf3",
   "execution_count": 476
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inferencing Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed83a62f7123fd5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def transformer_inference(batch_src_tensor, padding_mask_src_tensor, max_len=SEQ_LEN):\n",
    "    # feed the src tensor, padding mask tensor and tgt tensor including the bos token to the transformer\n",
    "    # loop until the model generate the eos token or the length of the tgt tensor is equal to max_len\n",
    "    # return the tgt tensor\n",
    "    tgt_tensor = torch.tensor([[tokenizer.bos_token_id]] * len(batch_src_tensor), device=device)\n",
    "    tgt_mask = torch.ones_like(tgt_tensor)\n",
    "    for _ in range(max_len):\n",
    "        logit = transformer(batch_src_tensor, tgt_tensor, padding_mask_src_tensor, tgt_mask)\n",
    "        logit = torch.softmax(logit, dim=-1)\n",
    "        pred_sents_ids = torch.argmax(logit, dim=-1)\n",
    "        # append the last token of the pred_sents_ids to the tgt_tensor\n",
    "        tgt_tensor = torch.cat([tgt_tensor, pred_sents_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "        tgt_mask = torch.ones_like(tgt_tensor)\n",
    "        if pred_sents_ids[-1][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "    return tgt_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:52:00.568543Z",
     "start_time": "2024-04-11T19:52:00.565575Z"
    }
   },
   "id": "9726152000f4223",
   "execution_count": 477
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ff9a8b15e8024d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    torch.save(model, file_path)\n",
    "    print(f\"Model saved at {file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:52:00.570817Z",
     "start_time": "2024-04-11T19:52:00.569221Z"
    }
   },
   "id": "b48b5a97b2734299",
   "execution_count": 478
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute BLEU Score Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d7a6e0ce549798c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def model_evaluate(model, dataloader, tokenizer, device, max_len=SEQ_LEN):\n",
    "    wmt14_en_de_test_tokenizer_dataset = WMT14ENDEDatasetHuggingFace(\n",
    "        en_raw_file_path= data_path + \"raw/test/newstest2015.en\",\n",
    "        de_raw_file_path= data_path + \"raw/test/newstest2015.de\",\n",
    "        device=device, max_len=SEQ_LEN, data_size=\"all\")\n",
    "    \n",
    "    test_dataloader = DataLoader(wmt14_en_de_test_tokenizer_dataset, batch_size=8, shuffle=True)\n",
    "    model.eval()\n",
    "    ref_sents_list = []\n",
    "    pred_sents_list = []\n",
    "    for step, data in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        batch_en_tensor = data[\"en_input_ids\"]\n",
    "        padding_mask_en_tensor = data[\"en_padding_mask\"]\n",
    "        ref_sents = data[\"de_sentence_str\"]\n",
    "        for i in range(max_len):\n",
    "            logit = model(batch_en_tensor, batch_de_tensor, padding_mask_en_tensor, padding_mask_de_tensor)\n",
    "            logit = torch.softmax(logit, dim=-1)\n",
    "            pred_sents_ids = torch.argmax(logit, dim=-1)\n",
    "            # append the last token of the pred_sents_ids to the tgt_tensor\n",
    "            batch_de_tensor = torch.cat([batch_de_tensor, pred_sents_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "            padding_mask_de_tensor = torch.ones_like(batch_de_tensor)\n",
    "            # if all the last token of the pred_sents_ids is padding token, then break\n",
    "           if torch.sum(pred_sents_ids[:, -1] == tokenizer.eos_token_id) == len(pred_sents_ids):\n",
    "               break\n",
    "        decoded_sents = tokenizer.batch_decode(batch_de_tensor, remove_special_tokens=True)\n",
    "        pred_sents_list.extend(decoded_sents)\n",
    "        ref_sents_list.extend([[ref_sent] for ref_sent in ref_sents])\n",
    "    blue_score = compute_bleu(ref_sents_list, pred_sents_list, smooth=True, max_order=4)\n",
    "    return blue_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T19:52:00.574167Z",
     "start_time": "2024-04-11T19:52:00.571536Z"
    }
   },
   "id": "16fc225f64486d60",
   "execution_count": 479
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad6d6a367dfb7c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:23<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.696127265691757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 7.618805259466171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:17<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 7.588914573192596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:19<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 7.507462605834007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:19<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 7.442336544394493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:21<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 7.401447296142578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 7.3422185480594635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:29<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 7.2796301394701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 7.18742161989212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:24<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 7.08918222784996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:20<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 6.936970919370651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 6.869780257344246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:23<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 6.801220208406448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:21<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 6.693325147032738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:24<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 6.560207307338715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:22<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 6.4635496735572815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:23<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss: 6.31735372543335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:19<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss: 6.275673478841782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:22<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Loss: 6.139788240194321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:24<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Loss: 6.062468200922012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 20/32 [00:12<00:07,  1.51it/s]"
     ]
    }
   ],
   "source": [
    "# read in the encoded train_en and train_de from \"../../data/translation/wmt14-en-de/tokenized/train/encoded_train.en\" and \"../../data/translation/wmt14-en-de/tokenized/train/\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for step, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # get the batch\n",
    "        batch_en_tensor = data[\"en_input_ids\"]\n",
    "        batch_de_tensor = data[\"de_input_ids\"]\n",
    "        padding_mask_en_tensor = data[\"en_padding_mask\"]\n",
    "        padding_mask_de_tensor = data[\"de_padding_mask\"]\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logit = transformer(batch_en_tensor, batch_de_tensor, padding_mask_en_tensor, padding_mask_de_tensor)\n",
    "        loss = criterion(logit.view(-1, VOCAB_SIZE), batch_de_tensor.view(-1))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if step % 30 == 0:\n",
    "            step_loss_list.append(loss.item())\n",
    "        if step == STEPS:\n",
    "            break\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss/len(dataloader)}\")\n",
    "    if REPORT_WANDB:\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": epoch_loss/len(dataloader)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if REPORT_WANDB:\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T19:52:00.574812Z"
    }
   },
   "id": "cf7183e6f128ee09",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92b1531bad47df7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "faf86dee3b06452b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
